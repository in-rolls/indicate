{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab/indicate/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3542qmUrVqO",
        "outputId": "8bae4e69-e5fe-425f-df03-dbd29656493d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab/indicate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1wHPGoNrrd2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    ##-------- LSTM layer in Encoder ------- ##\n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, x, hidden, training=None):\n",
        "    x = self.embedding(x)\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
        "    return output, h, c\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "metadata": {
        "id": "JfjRUNuuniGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,\n",
        "                 max_length_input, max_length_output, attention_type='luong'):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.attention_type = attention_type\n",
        "        self.max_length_input = max_length_input\n",
        "        self.max_length_output = max_length_output\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Final Dense layer on which softmax will be applied\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # Define the fundamental cell for decoder recurrent structure\n",
        "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "        self.decoder_rnn = tf.keras.layers.RNN(self.decoder_rnn_cell, return_sequences=True, return_state=True)\n",
        "\n",
        "        # Create attention mechanism\n",
        "        # For Luong attention: project the query to match the encoder output's dimension (dec_units)\n",
        "        if self.attention_type == 'luong':\n",
        "            self.query_layer = tf.keras.layers.Dense(dec_units)\n",
        "\n",
        "        self.attention_mechanism = self.build_attention_mechanism(None)\n",
        "\n",
        "    def build_attention_mechanism(self, memory):\n",
        "        if self.attention_type == 'bahdanau':\n",
        "            return tf.keras.layers.AdditiveAttention()\n",
        "        else:\n",
        "            return tf.keras.layers.Attention()\n",
        "\n",
        "    def call(self, inputs, initial_state, encoder_outputs):\n",
        "        # Get the embeddings of the inputs\n",
        "        x = self.embedding(inputs)\n",
        "\n",
        "        if self.attention_type == 'luong':\n",
        "          query = self.query_layer(x)\n",
        "        else:\n",
        "          query = x\n",
        "\n",
        "        # Feature dimensions to the attention layer\n",
        "        attention_output, attention_weights = self.attention_mechanism([query, encoder_outputs], return_attention_scores=True)\n",
        "\n",
        "        # Concatenate the attention output with the LSTM Cell output\n",
        "        lstm_input = tf.concat([attention_output, x], axis=-1)\n",
        "\n",
        "        # Process through the LSTM cell\n",
        "        outputs, state_h, state_c = self.decoder_rnn(lstm_input, initial_state)\n",
        "\n",
        "        # Pass through the final dense layer\n",
        "        outputs = self.fc(outputs)\n",
        "\n",
        "        return outputs, (state_h, state_c), attention_weights\n",
        "\n",
        "    def build_initial_state(self, batch_sz, encoder_state):\n",
        "        hidden_state, cell_state = encoder_state\n",
        "        return [hidden_state, cell_state]"
      ],
      "metadata": {
        "id": "oC_ggIudwc5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from sequence (mapping back)\n",
        "def sequence_to_chars(tokenizer, sequence):\n",
        "    word_index = tokenizer.word_index\n",
        "    reverse_map = {val: key for key, val in word_index.items()}\n",
        "    retext = ''\n",
        "    for q in sequence:\n",
        "        if q != 0:\n",
        "            retext += reverse_map[q]\n",
        "    return retext\n",
        "\n",
        "def evaluate_sentence(sentence, units, input_lang_tokenizer, target_lang_tokenizer,\n",
        "                      encoder, decoder, max_length_input):\n",
        "    # sentence = '^' + sentence.strip() + '$'\n",
        "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                            maxlen=max_length_input,\n",
        "                                                            padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    inference_batch_size = inputs.shape[0]\n",
        "    result = ''\n",
        "\n",
        "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size, units))]\n",
        "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "    dec_h = enc_h\n",
        "    dec_c = enc_c\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size], target_lang_tokenizer.word_index['^'])\n",
        "    end_token = target_lang_tokenizer.word_index['$']\n",
        "\n",
        "    # Use a simple greedy decoding method\n",
        "    outputs = []\n",
        "    for t in range(decoder.max_length_output):\n",
        "        # The current input is the last output from previous time step\n",
        "        if t == 0:\n",
        "            input_dec = start_tokens\n",
        "        else:\n",
        "            input_dec = tf.expand_dims(sample_ids, 1)\n",
        "\n",
        "        # Call the decoder\n",
        "        predictions, (dec_h, dec_c), _ = decoder(input_dec, (dec_h, dec_c), enc_out)\n",
        "\n",
        "        # Sample the next token\n",
        "        sample_ids = tf.argmax(predictions, axis=-1).numpy()[:, -1]\n",
        "        outputs.append(sample_ids)\n",
        "\n",
        "    return tf.concat(outputs, axis=0)\n",
        "\n",
        "def translate(sentence, units, input_lang_tokenizer, target_lang_tokenizer, encoder, decoder, max_length_input):\n",
        "    result = evaluate_sentence(sentence, units, input_lang_tokenizer, target_lang_tokenizer, encoder, decoder, max_length_input)\n",
        "    result = sequence_to_chars(target_lang_tokenizer, result[0])\n",
        "    return result.strip('$')"
      ],
      "metadata": {
        "id": "pnvLyemcnozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKh1Ecloa9Pz",
        "outputId": "a4e02f65-cc04-4585-9651-9db9a739515b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "a1wzzI6zrsQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_target_df = pd.read_csv('./data/hindi.csv')"
      ],
      "metadata": {
        "id": "63PtIy_-rGtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for training"
      ],
      "metadata": {
        "id": "jLfWz0d0ryHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Append start and end of sequence for target"
      ],
      "metadata": {
        "id": "RbXARDGdsByb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# append start and end of sequence for target\n",
        "sos = '^'\n",
        "eos = '$'\n",
        "\n",
        "input_target_df['english'] = sos + input_target_df['english'].astype(str) + eos"
      ],
      "metadata": {
        "id": "lqEt-Smdrxas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_target_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7SH3SP1Xsbp1",
        "outputId": "f5bd5bcb-c53e-44ee-9b21-77d058a12c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     hindi                     english\n",
              "0  प्रयात सुबोध चन्द्र दास   ^late subodh chandra das$\n",
              "1    स्वर्गीय पिनाकि कुंडू         ^late pinaki kundu$\n",
              "2   मृत नूर महम्मद बिश्वास  ^late nur mohammad biswas$\n",
              "3         नारायण चंद्र डॉं      ^narayan chandra dawn$\n",
              "4            आब्दुल मन्नान              ^abdul mannan$"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6cc869bf-2284-4e79-9db2-710c93dd4b8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>प्रयात सुबोध चन्द्र दास</td>\n",
              "      <td>^late subodh chandra das$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>स्वर्गीय पिनाकि कुंडू</td>\n",
              "      <td>^late pinaki kundu$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>मृत नूर महम्मद बिश्वास</td>\n",
              "      <td>^late nur mohammad biswas$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>नारायण चंद्र डॉं</td>\n",
              "      <td>^narayan chandra dawn$</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>आब्दुल मन्नान</td>\n",
              "      <td>^abdul mannan$</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6cc869bf-2284-4e79-9db2-710c93dd4b8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6cc869bf-2284-4e79-9db2-710c93dd4b8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6cc869bf-2284-4e79-9db2-710c93dd4b8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f3ad6501-6696-466b-99ae-6373b70f380e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3ad6501-6696-466b-99ae-6373b70f380e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f3ad6501-6696-466b-99ae-6373b70f380e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "input_target_df"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Vocab for Source and Target"
      ],
      "metadata": {
        "id": "LobqH0_ysdXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_words =  input_target_df['hindi'].tolist()\n",
        "target_words =  input_target_df['english'].tolist()"
      ],
      "metadata": {
        "id": "-dRUCb3dsLn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total number of input words {len(input_words)}\")\n",
        "print(f\"Total number of target words {len(target_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0_y53xksY4Y",
        "outputId": "51f88d98-a55a-4781-a9fa-92e0bb7d1202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of input words 371035\n",
            "Total number of target words 371035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "input_lang_tokenizer.fit_on_texts(input_words)\n",
        "input_tensor = input_lang_tokenizer.texts_to_sequences(input_words)\n",
        "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, padding='post')"
      ],
      "metadata": {
        "id": "zKIjiRn_suXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level=True)\n",
        "target_lang_tokenizer.fit_on_texts(target_words)\n",
        "target_tensor = target_lang_tokenizer.texts_to_sequences(target_words)\n",
        "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post')"
      ],
      "metadata": {
        "id": "GAUAkBKZs_B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Vocab for Source and Target"
      ],
      "metadata": {
        "id": "xbx21xm5tKG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenizers\n",
        "hindi_tokenizer_json = input_lang_tokenizer.to_json()\n",
        "with open('hindi_tokens.json', 'w') as f:\n",
        "  json.dump(hindi_tokenizer_json, f)\n",
        "\n",
        "english_tokenizer_json = target_lang_tokenizer.to_json()\n",
        "with open('english_tokens.json', 'w') as f:\n",
        "  json.dump(english_tokenizer_json, f)"
      ],
      "metadata": {
        "id": "N_tEU-mctE64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into Train and Validation"
      ],
      "metadata": {
        "id": "YP6tOVP_tglo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train and validation\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "metadata": {
        "id": "XuednRVRtVPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 120000\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "8euCEPBKtxrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecn7UTUAt5dX",
        "outputId": "69054e78-1b57-4c2d-bf63-51cf39efed77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 47]), TensorShape([64, 173]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of prepared dataset"
      ],
      "metadata": {
        "id": "BncLpNAEuWP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_inp_size = len(input_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_lang_tokenizer.word_index)+1\n",
        "\n",
        "max_length_input = input_tensor.shape[1]\n",
        "max_length_output = target_tensor.shape[1]\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "steps_per_epoch = input_tensor.shape[0]//BATCH_SIZE"
      ],
      "metadata": {
        "id": "7UzIiLwTuG3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of samples:', len(input_words))\n",
        "\n",
        "print('Input Vocab size:', vocab_inp_size)\n",
        "print('Target Vocab size:', vocab_tar_size)\n",
        "\n",
        "print('Max sequence length for inputs:', max_length_input)\n",
        "print('Max sequence length for outputs:', max_length_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3NgN0bRuhWQ",
        "outputId": "6c29c81a-3162-4f94-adbb-48e255c772f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 371035\n",
            "Input Vocab size: 200\n",
            "Target Vocab size: 137\n",
            "Max sequence length for inputs: 47\n",
            "Max sequence length for outputs: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "Qr9di8mBupTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_jOmoDVEutZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Encoder"
      ],
      "metadata": {
        "id": "zxMmclrzuyJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Encoder Stack\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))\n",
        "print('Encoder output vector shape: (batch size, sequence length, units) {}'.format(sample_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlNRs98cuxdp",
        "outputId": "d71bd178-ff67-4721-c43c-d6abe3100454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 47, 1024)\n",
            "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
            "Encoder c vector shape: (batch size, units) (64, 1024)\n",
            "Encoder output vector shape: (batch size, sequence length, units) (64, 47, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "Qw4ebubBu7MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE,\n",
        "                  max_length_input, max_length_output, 'luong')"
      ],
      "metadata": {
        "id": "EFLPrhRVu9hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Decoder"
      ],
      "metadata": {
        "id": "-bDN2S0TvEIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test decoder stack\n",
        "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c])\n",
        "\n",
        "\n",
        "sample_decoder_outputs = decoder(sample_x, initial_state, sample_output)\n",
        "\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTNciGVdvCzs",
        "outputId": "365a06cb-ff2b-43e7-a4f9-6012c0fb764f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Outputs Shape:  (64, 173, 137)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Loss Function"
      ],
      "metadata": {
        "id": "Xh07QJ3AwEkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "GAN_YevuwH9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoints"
      ],
      "metadata": {
        "id": "n04SWU2lwMAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './data/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "-RchUEMMwLTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One train_step operations"
      ],
      "metadata": {
        "id": "kn92tVRdcvb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "    # For the decoder, the input is the target sequence excluding the <end> token;\n",
        "    # and the “real” sequence is target shifted by one (excluding the <start> token).\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "\n",
        "    # Build the initial state for the decoder.\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c])\n",
        "\n",
        "    # Unpack the outputs from the decoder.\n",
        "    logits, decoder_state, attention_weights = decoder(dec_input, decoder_initial_state, enc_output)\n",
        "    # Compute the loss from logits and the true labels.\n",
        "    loss = loss_function(real, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "Z4wTBov2c1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Step (no gradient tape needed)\n",
        "@tf.function\n",
        "def val_step(inp, targ, enc_hidden):\n",
        "  # Run the encoder on the validation input.\n",
        "  enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "  # Prepare the decoder input and targets as in training.\n",
        "  dec_input = targ[:, :-1]\n",
        "  real = targ[:, 1:]\n",
        "  # Build the initial state for the decoder.\n",
        "  decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c])\n",
        "  # Get the logits from the decoder.\n",
        "  logits, _, _ = decoder(dec_input, decoder_initial_state, enc_output)\n",
        "  # Compute and return the loss.\n",
        "  loss = loss_function(real, logits)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "3eZgjJnxOMTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Jv13BMMfc5rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 25\n",
        "patience = 5\n",
        "epochs_without_improvement = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "best_loss = 1\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # -- Training phase --\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_train_loss = 0\n",
        "  # loop over the training batches\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_train_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "\n",
        "  loss = total_train_loss / steps_per_epoch\n",
        "\n",
        "\n",
        "  # -- validation phase --\n",
        "  total_val_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(val_dataset):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    batch_val_loss = val_step(inp, targ, enc_hidden)\n",
        "    total_val_loss += batch_val_loss\n",
        "\n",
        "  val_loss = total_val_loss / len(val_dataset)\n",
        "  print('Epoch {} Loss {:.4f} Val Loss {:.4f}'.format(epoch + 1,\n",
        "                                                       loss,\n",
        "                                                       val_loss))\n",
        "\n",
        "  # -- checkpoint & early stopping --\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    print('Saving checkpoint for epoch {} Loss {:.4f}'.format(epoch + 1, val_loss))\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    epochs_without_improvement = 0\n",
        "  else:\n",
        "    epochs_without_improvement += 1\n",
        "\n",
        "  # break the loop if there is no improvement\n",
        "  if epochs_without_improvement >= patience:\n",
        "    print(f\"Early stopping triggered after {patience} consecutive epochs w/o improvement\")\n",
        "    break\n",
        "\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZheDY-jc7P7",
        "outputId": "4b1bd2dd-4bdf-426b-a062-bb4ae0dc433e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 0.2329\n",
            "Epoch 1 Batch 100 Loss 0.1237\n",
            "Epoch 1 Batch 200 Loss 0.0989\n",
            "Epoch 1 Batch 300 Loss 0.0988\n",
            "Epoch 1 Batch 400 Loss 0.0887\n",
            "Epoch 1 Batch 500 Loss 0.0944\n",
            "Epoch 1 Batch 600 Loss 0.0800\n",
            "Epoch 1 Batch 700 Loss 0.0663\n",
            "Epoch 1 Batch 800 Loss 0.0466\n",
            "Epoch 1 Batch 900 Loss 0.0326\n",
            "Epoch 1 Batch 1000 Loss 0.0208\n",
            "Epoch 1 Batch 1100 Loss 0.0239\n",
            "Epoch 1 Batch 1200 Loss 0.0202\n",
            "Epoch 1 Batch 1300 Loss 0.0175\n",
            "Epoch 1 Batch 1400 Loss 0.0211\n",
            "Epoch 1 Batch 1500 Loss 0.0181\n",
            "Epoch 1 Batch 1600 Loss 0.0248\n",
            "Epoch 1 Batch 1700 Loss 0.0215\n",
            "Epoch 1 Batch 1800 Loss 0.0208\n",
            "Epoch 1 Batch 1900 Loss 0.0168\n",
            "Epoch 1 Batch 2000 Loss 0.0163\n",
            "Epoch 1 Batch 2100 Loss 0.0212\n",
            "Epoch 1 Batch 2200 Loss 0.0224\n",
            "Epoch 1 Batch 2300 Loss 0.0178\n",
            "Epoch 1 Batch 2400 Loss 0.0144\n",
            "Epoch 1 Batch 2500 Loss 0.0170\n",
            "Epoch 1 Batch 2600 Loss 0.0156\n",
            "Epoch 1 Batch 2700 Loss 0.0208\n",
            "Epoch 1 Batch 2800 Loss 0.0179\n",
            "Epoch 1 Batch 2900 Loss 0.0169\n",
            "Epoch 1 Batch 3000 Loss 0.0161\n",
            "Epoch 1 Batch 3100 Loss 0.0189\n",
            "Epoch 1 Batch 3200 Loss 0.0132\n",
            "Epoch 1 Batch 3300 Loss 0.0180\n",
            "Epoch 1 Batch 3400 Loss 0.0127\n",
            "Epoch 1 Batch 3500 Loss 0.0159\n",
            "Epoch 1 Batch 3600 Loss 0.0147\n",
            "Epoch 1 Batch 3700 Loss 0.0175\n",
            "Epoch 1 Batch 3800 Loss 0.0137\n",
            "Epoch 1 Batch 3900 Loss 0.0175\n",
            "Epoch 1 Batch 4000 Loss 0.0162\n",
            "Epoch 1 Batch 4100 Loss 0.0151\n",
            "Epoch 1 Batch 4200 Loss 0.0243\n",
            "Epoch 1 Batch 4300 Loss 0.0127\n",
            "Epoch 1 Batch 4400 Loss 0.0201\n",
            "Epoch 1 Batch 4500 Loss 0.0172\n",
            "Epoch 1 Batch 4600 Loss 0.0129\n",
            "Epoch 1 Loss 0.0257 Val Loss 0.0154\n",
            "Saving checkpoint for epoch 1 Loss 0.0154\n",
            "Time taken for 1 epoch 738.509771823883 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.0162\n",
            "Epoch 2 Batch 100 Loss 0.0158\n",
            "Epoch 2 Batch 200 Loss 0.0142\n",
            "Epoch 2 Batch 300 Loss 0.0158\n",
            "Epoch 2 Batch 400 Loss 0.0136\n",
            "Epoch 2 Batch 500 Loss 0.0154\n",
            "Epoch 2 Batch 600 Loss 0.0178\n",
            "Epoch 2 Batch 700 Loss 0.0231\n",
            "Epoch 2 Batch 800 Loss 0.0108\n",
            "Epoch 2 Batch 900 Loss 0.0191\n",
            "Epoch 2 Batch 1000 Loss 0.0166\n",
            "Epoch 2 Batch 1100 Loss 0.0155\n",
            "Epoch 2 Batch 1200 Loss 0.0133\n",
            "Epoch 2 Batch 1300 Loss 0.0135\n",
            "Epoch 2 Batch 1400 Loss 0.0152\n",
            "Epoch 2 Batch 1500 Loss 0.0171\n",
            "Epoch 2 Batch 1600 Loss 0.0114\n",
            "Epoch 2 Batch 1700 Loss 0.0130\n",
            "Epoch 2 Batch 1800 Loss 0.0176\n",
            "Epoch 2 Batch 1900 Loss 0.0146\n",
            "Epoch 2 Batch 2000 Loss 0.0141\n",
            "Epoch 2 Batch 2100 Loss 0.0084\n",
            "Epoch 2 Batch 2200 Loss 0.0137\n",
            "Epoch 2 Batch 2300 Loss 0.0157\n",
            "Epoch 2 Batch 2400 Loss 0.0119\n",
            "Epoch 2 Batch 2500 Loss 0.0161\n",
            "Epoch 2 Batch 2600 Loss 0.0184\n",
            "Epoch 2 Batch 2700 Loss 0.0114\n",
            "Epoch 2 Batch 2800 Loss 0.0123\n",
            "Epoch 2 Batch 2900 Loss 0.0166\n",
            "Epoch 2 Batch 3000 Loss 0.0149\n",
            "Epoch 2 Batch 3100 Loss 0.0179\n",
            "Epoch 2 Batch 3200 Loss 0.0171\n",
            "Epoch 2 Batch 3300 Loss 0.0137\n",
            "Epoch 2 Batch 3400 Loss 0.0194\n",
            "Epoch 2 Batch 3500 Loss 0.0135\n",
            "Epoch 2 Batch 3600 Loss 0.0117\n",
            "Epoch 2 Batch 3700 Loss 0.0157\n",
            "Epoch 2 Batch 3800 Loss 0.0109\n",
            "Epoch 2 Batch 3900 Loss 0.0160\n",
            "Epoch 2 Batch 4000 Loss 0.0122\n",
            "Epoch 2 Batch 4100 Loss 0.0119\n",
            "Epoch 2 Batch 4200 Loss 0.0159\n",
            "Epoch 2 Batch 4300 Loss 0.0130\n",
            "Epoch 2 Batch 4400 Loss 0.0117\n",
            "Epoch 2 Batch 4500 Loss 0.0095\n",
            "Epoch 2 Batch 4600 Loss 0.0123\n",
            "Epoch 2 Loss 0.0112 Val Loss 0.0137\n",
            "Saving checkpoint for epoch 2 Loss 0.0137\n",
            "Time taken for 1 epoch 733.146365404129 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0163\n",
            "Epoch 3 Batch 100 Loss 0.0084\n",
            "Epoch 3 Batch 200 Loss 0.0094\n",
            "Epoch 3 Batch 300 Loss 0.0172\n",
            "Epoch 3 Batch 400 Loss 0.0141\n",
            "Epoch 3 Batch 500 Loss 0.0119\n",
            "Epoch 3 Batch 600 Loss 0.0108\n",
            "Epoch 3 Batch 700 Loss 0.0114\n",
            "Epoch 3 Batch 800 Loss 0.0096\n",
            "Epoch 3 Batch 900 Loss 0.0154\n",
            "Epoch 3 Batch 1000 Loss 0.0098\n",
            "Epoch 3 Batch 1100 Loss 0.0109\n",
            "Epoch 3 Batch 1200 Loss 0.0156\n",
            "Epoch 3 Batch 1300 Loss 0.0125\n",
            "Epoch 3 Batch 1400 Loss 0.0154\n",
            "Epoch 3 Batch 1500 Loss 0.0140\n",
            "Epoch 3 Batch 1600 Loss 0.0116\n",
            "Epoch 3 Batch 1700 Loss 0.0091\n",
            "Epoch 3 Batch 1800 Loss 0.0131\n",
            "Epoch 3 Batch 1900 Loss 0.0123\n",
            "Epoch 3 Batch 2000 Loss 0.0103\n",
            "Epoch 3 Batch 2100 Loss 0.0129\n",
            "Epoch 3 Batch 2200 Loss 0.0098\n",
            "Epoch 3 Batch 2300 Loss 0.0124\n",
            "Epoch 3 Batch 2400 Loss 0.0107\n",
            "Epoch 3 Batch 2500 Loss 0.0152\n",
            "Epoch 3 Batch 2600 Loss 0.0131\n",
            "Epoch 3 Batch 2700 Loss 0.0114\n",
            "Epoch 3 Batch 2800 Loss 0.0116\n",
            "Epoch 3 Batch 2900 Loss 0.0119\n",
            "Epoch 3 Batch 3000 Loss 0.0151\n",
            "Epoch 3 Batch 3100 Loss 0.0115\n",
            "Epoch 3 Batch 3200 Loss 0.0115\n",
            "Epoch 3 Batch 3300 Loss 0.0208\n",
            "Epoch 3 Batch 3400 Loss 0.0084\n",
            "Epoch 3 Batch 3500 Loss 0.0082\n",
            "Epoch 3 Batch 3600 Loss 0.0158\n",
            "Epoch 3 Batch 3700 Loss 0.0125\n",
            "Epoch 3 Batch 3800 Loss 0.0130\n",
            "Epoch 3 Batch 3900 Loss 0.0103\n",
            "Epoch 3 Batch 4000 Loss 0.0152\n",
            "Epoch 3 Batch 4100 Loss 0.0118\n",
            "Epoch 3 Batch 4200 Loss 0.0122\n",
            "Epoch 3 Batch 4300 Loss 0.0097\n",
            "Epoch 3 Batch 4400 Loss 0.0147\n",
            "Epoch 3 Batch 4500 Loss 0.0107\n",
            "Epoch 3 Batch 4600 Loss 0.0094\n",
            "Epoch 3 Loss 0.0097 Val Loss 0.0130\n",
            "Saving checkpoint for epoch 3 Loss 0.0130\n",
            "Time taken for 1 epoch 733.5029106140137 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0120\n",
            "Epoch 4 Batch 100 Loss 0.0107\n",
            "Epoch 4 Batch 200 Loss 0.0120\n",
            "Epoch 4 Batch 300 Loss 0.0105\n",
            "Epoch 4 Batch 400 Loss 0.0117\n",
            "Epoch 4 Batch 500 Loss 0.0090\n",
            "Epoch 4 Batch 600 Loss 0.0118\n",
            "Epoch 4 Batch 700 Loss 0.0096\n",
            "Epoch 4 Batch 800 Loss 0.0085\n",
            "Epoch 4 Batch 900 Loss 0.0101\n",
            "Epoch 4 Batch 1000 Loss 0.0104\n",
            "Epoch 4 Batch 1100 Loss 0.0163\n",
            "Epoch 4 Batch 1200 Loss 0.0126\n",
            "Epoch 4 Batch 1300 Loss 0.0115\n",
            "Epoch 4 Batch 1400 Loss 0.0114\n",
            "Epoch 4 Batch 1500 Loss 0.0138\n",
            "Epoch 4 Batch 1600 Loss 0.0092\n",
            "Epoch 4 Batch 1700 Loss 0.0076\n",
            "Epoch 4 Batch 1800 Loss 0.0109\n",
            "Epoch 4 Batch 1900 Loss 0.0059\n",
            "Epoch 4 Batch 2000 Loss 0.0131\n",
            "Epoch 4 Batch 2100 Loss 0.0106\n",
            "Epoch 4 Batch 2200 Loss 0.0101\n",
            "Epoch 4 Batch 2300 Loss 0.0084\n",
            "Epoch 4 Batch 2400 Loss 0.0104\n",
            "Epoch 4 Batch 2500 Loss 0.0152\n",
            "Epoch 4 Batch 2600 Loss 0.0105\n",
            "Epoch 4 Batch 2700 Loss 0.0127\n",
            "Epoch 4 Batch 2800 Loss 0.0113\n",
            "Epoch 4 Batch 2900 Loss 0.0082\n",
            "Epoch 4 Batch 3000 Loss 0.0187\n",
            "Epoch 4 Batch 3100 Loss 0.0087\n",
            "Epoch 4 Batch 3200 Loss 0.0091\n",
            "Epoch 4 Batch 3300 Loss 0.0106\n",
            "Epoch 4 Batch 3400 Loss 0.0085\n",
            "Epoch 4 Batch 3500 Loss 0.0134\n",
            "Epoch 4 Batch 3600 Loss 0.0098\n",
            "Epoch 4 Batch 3700 Loss 0.0118\n",
            "Epoch 4 Batch 3800 Loss 0.0115\n",
            "Epoch 4 Batch 3900 Loss 0.0092\n",
            "Epoch 4 Batch 4000 Loss 0.0122\n",
            "Epoch 4 Batch 4100 Loss 0.0087\n",
            "Epoch 4 Batch 4200 Loss 0.0114\n",
            "Epoch 4 Batch 4300 Loss 0.0128\n",
            "Epoch 4 Batch 4400 Loss 0.0097\n",
            "Epoch 4 Batch 4500 Loss 0.0119\n",
            "Epoch 4 Batch 4600 Loss 0.0118\n",
            "Epoch 4 Loss 0.0086 Val Loss 0.0128\n",
            "Saving checkpoint for epoch 4 Loss 0.0128\n",
            "Time taken for 1 epoch 731.1751847267151 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0082\n",
            "Epoch 5 Batch 100 Loss 0.0091\n",
            "Epoch 5 Batch 200 Loss 0.0114\n",
            "Epoch 5 Batch 300 Loss 0.0095\n",
            "Epoch 5 Batch 400 Loss 0.0071\n",
            "Epoch 5 Batch 500 Loss 0.0109\n",
            "Epoch 5 Batch 600 Loss 0.0102\n",
            "Epoch 5 Batch 700 Loss 0.0097\n",
            "Epoch 5 Batch 800 Loss 0.0060\n",
            "Epoch 5 Batch 900 Loss 0.0086\n",
            "Epoch 5 Batch 1000 Loss 0.0097\n",
            "Epoch 5 Batch 1100 Loss 0.0083\n",
            "Epoch 5 Batch 1200 Loss 0.0115\n",
            "Epoch 5 Batch 1300 Loss 0.0075\n",
            "Epoch 5 Batch 1400 Loss 0.0068\n",
            "Epoch 5 Batch 1500 Loss 0.0091\n",
            "Epoch 5 Batch 1600 Loss 0.0093\n",
            "Epoch 5 Batch 1700 Loss 0.0092\n",
            "Epoch 5 Batch 1800 Loss 0.0078\n",
            "Epoch 5 Batch 1900 Loss 0.0076\n",
            "Epoch 5 Batch 2000 Loss 0.0076\n",
            "Epoch 5 Batch 2100 Loss 0.0096\n",
            "Epoch 5 Batch 2200 Loss 0.0087\n",
            "Epoch 5 Batch 2300 Loss 0.0080\n",
            "Epoch 5 Batch 2400 Loss 0.0097\n",
            "Epoch 5 Batch 2500 Loss 0.0082\n",
            "Epoch 5 Batch 2600 Loss 0.0111\n",
            "Epoch 5 Batch 2700 Loss 0.0106\n",
            "Epoch 5 Batch 2800 Loss 0.0113\n",
            "Epoch 5 Batch 2900 Loss 0.0162\n",
            "Epoch 5 Batch 3000 Loss 0.0081\n",
            "Epoch 5 Batch 3100 Loss 0.0067\n",
            "Epoch 5 Batch 3200 Loss 0.0107\n",
            "Epoch 5 Batch 3300 Loss 0.0091\n",
            "Epoch 5 Batch 3400 Loss 0.0113\n",
            "Epoch 5 Batch 3500 Loss 0.0120\n",
            "Epoch 5 Batch 3600 Loss 0.0099\n",
            "Epoch 5 Batch 3700 Loss 0.0094\n",
            "Epoch 5 Batch 3800 Loss 0.0079\n",
            "Epoch 5 Batch 3900 Loss 0.0115\n",
            "Epoch 5 Batch 4000 Loss 0.0099\n",
            "Epoch 5 Batch 4100 Loss 0.0118\n",
            "Epoch 5 Batch 4200 Loss 0.0086\n",
            "Epoch 5 Batch 4300 Loss 0.0104\n",
            "Epoch 5 Batch 4400 Loss 0.0069\n",
            "Epoch 5 Batch 4500 Loss 0.0106\n",
            "Epoch 5 Batch 4600 Loss 0.0087\n",
            "Epoch 5 Loss 0.0078 Val Loss 0.0128\n",
            "Saving checkpoint for epoch 5 Loss 0.0128\n",
            "Time taken for 1 epoch 730.5528981685638 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0103\n",
            "Epoch 6 Batch 100 Loss 0.0072\n",
            "Epoch 6 Batch 200 Loss 0.0085\n",
            "Epoch 6 Batch 300 Loss 0.0106\n",
            "Epoch 6 Batch 400 Loss 0.0118\n",
            "Epoch 6 Batch 500 Loss 0.0084\n",
            "Epoch 6 Batch 600 Loss 0.0081\n",
            "Epoch 6 Batch 700 Loss 0.0099\n",
            "Epoch 6 Batch 800 Loss 0.0110\n",
            "Epoch 6 Batch 900 Loss 0.0089\n",
            "Epoch 6 Batch 1000 Loss 0.0082\n",
            "Epoch 6 Batch 1100 Loss 0.0108\n",
            "Epoch 6 Batch 1200 Loss 0.0076\n",
            "Epoch 6 Batch 1300 Loss 0.0099\n",
            "Epoch 6 Batch 1400 Loss 0.0101\n",
            "Epoch 6 Batch 1500 Loss 0.0097\n",
            "Epoch 6 Batch 1600 Loss 0.0076\n",
            "Epoch 6 Batch 1700 Loss 0.0062\n",
            "Epoch 6 Batch 1800 Loss 0.0090\n",
            "Epoch 6 Batch 1900 Loss 0.0123\n",
            "Epoch 6 Batch 2000 Loss 0.0083\n",
            "Epoch 6 Batch 2100 Loss 0.0074\n",
            "Epoch 6 Batch 2200 Loss 0.0075\n",
            "Epoch 6 Batch 2300 Loss 0.0099\n",
            "Epoch 6 Batch 2400 Loss 0.0066\n",
            "Epoch 6 Batch 2500 Loss 0.0111\n",
            "Epoch 6 Batch 2600 Loss 0.0074\n",
            "Epoch 6 Batch 2700 Loss 0.0082\n",
            "Epoch 6 Batch 2800 Loss 0.0113\n",
            "Epoch 6 Batch 2900 Loss 0.0095\n",
            "Epoch 6 Batch 3000 Loss 0.0058\n",
            "Epoch 6 Batch 3100 Loss 0.0077\n",
            "Epoch 6 Batch 3200 Loss 0.0078\n",
            "Epoch 6 Batch 3300 Loss 0.0114\n",
            "Epoch 6 Batch 3400 Loss 0.0086\n",
            "Epoch 6 Batch 3500 Loss 0.0070\n",
            "Epoch 6 Batch 3600 Loss 0.0082\n",
            "Epoch 6 Batch 3700 Loss 0.0081\n",
            "Epoch 6 Batch 3800 Loss 0.0077\n",
            "Epoch 6 Batch 3900 Loss 0.0105\n",
            "Epoch 6 Batch 4000 Loss 0.0098\n",
            "Epoch 6 Batch 4100 Loss 0.0104\n",
            "Epoch 6 Batch 4200 Loss 0.0095\n",
            "Epoch 6 Batch 4300 Loss 0.0084\n",
            "Epoch 6 Batch 4400 Loss 0.0097\n",
            "Epoch 6 Batch 4500 Loss 0.0083\n",
            "Epoch 6 Batch 4600 Loss 0.0049\n",
            "Epoch 6 Loss 0.0070 Val Loss 0.0131\n",
            "Time taken for 1 epoch 731.5087742805481 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0111\n",
            "Epoch 7 Batch 100 Loss 0.0057\n",
            "Epoch 7 Batch 200 Loss 0.0063\n",
            "Epoch 7 Batch 300 Loss 0.0078\n",
            "Epoch 7 Batch 400 Loss 0.0109\n",
            "Epoch 7 Batch 500 Loss 0.0087\n",
            "Epoch 7 Batch 600 Loss 0.0088\n",
            "Epoch 7 Batch 700 Loss 0.0054\n",
            "Epoch 7 Batch 800 Loss 0.0075\n",
            "Epoch 7 Batch 900 Loss 0.0118\n",
            "Epoch 7 Batch 1000 Loss 0.0082\n",
            "Epoch 7 Batch 1100 Loss 0.0073\n",
            "Epoch 7 Batch 1200 Loss 0.0075\n",
            "Epoch 7 Batch 1300 Loss 0.0089\n",
            "Epoch 7 Batch 1400 Loss 0.0099\n",
            "Epoch 7 Batch 1500 Loss 0.0066\n",
            "Epoch 7 Batch 1600 Loss 0.0061\n",
            "Epoch 7 Batch 1700 Loss 0.0083\n",
            "Epoch 7 Batch 1800 Loss 0.0092\n",
            "Epoch 7 Batch 1900 Loss 0.0071\n",
            "Epoch 7 Batch 2000 Loss 0.0103\n",
            "Epoch 7 Batch 2100 Loss 0.0052\n",
            "Epoch 7 Batch 2200 Loss 0.0095\n",
            "Epoch 7 Batch 2300 Loss 0.0063\n",
            "Epoch 7 Batch 2400 Loss 0.0081\n",
            "Epoch 7 Batch 2500 Loss 0.0071\n",
            "Epoch 7 Batch 2600 Loss 0.0077\n",
            "Epoch 7 Batch 2700 Loss 0.0091\n",
            "Epoch 7 Batch 2800 Loss 0.0073\n",
            "Epoch 7 Batch 2900 Loss 0.0070\n",
            "Epoch 7 Batch 3000 Loss 0.0073\n",
            "Epoch 7 Batch 3100 Loss 0.0057\n",
            "Epoch 7 Batch 3200 Loss 0.0074\n",
            "Epoch 7 Batch 3300 Loss 0.0071\n",
            "Epoch 7 Batch 3400 Loss 0.0074\n",
            "Epoch 7 Batch 3500 Loss 0.0079\n",
            "Epoch 7 Batch 3600 Loss 0.0089\n",
            "Epoch 7 Batch 3700 Loss 0.0070\n",
            "Epoch 7 Batch 3800 Loss 0.0072\n",
            "Epoch 7 Batch 3900 Loss 0.0077\n",
            "Epoch 7 Batch 4000 Loss 0.0080\n",
            "Epoch 7 Batch 4100 Loss 0.0083\n",
            "Epoch 7 Batch 4200 Loss 0.0081\n",
            "Epoch 7 Batch 4300 Loss 0.0090\n",
            "Epoch 7 Batch 4400 Loss 0.0077\n",
            "Epoch 7 Batch 4500 Loss 0.0072\n",
            "Epoch 7 Batch 4600 Loss 0.0119\n",
            "Epoch 7 Loss 0.0064 Val Loss 0.0133\n",
            "Time taken for 1 epoch 729.9929449558258 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0066\n",
            "Epoch 8 Batch 100 Loss 0.0049\n",
            "Epoch 8 Batch 200 Loss 0.0059\n",
            "Epoch 8 Batch 300 Loss 0.0073\n",
            "Epoch 8 Batch 400 Loss 0.0081\n",
            "Epoch 8 Batch 500 Loss 0.0062\n",
            "Epoch 8 Batch 600 Loss 0.0069\n",
            "Epoch 8 Batch 700 Loss 0.0062\n",
            "Epoch 8 Batch 800 Loss 0.0086\n",
            "Epoch 8 Batch 900 Loss 0.0062\n",
            "Epoch 8 Batch 1000 Loss 0.0051\n",
            "Epoch 8 Batch 1100 Loss 0.0068\n",
            "Epoch 8 Batch 1200 Loss 0.0051\n",
            "Epoch 8 Batch 1300 Loss 0.0084\n",
            "Epoch 8 Batch 1400 Loss 0.0056\n",
            "Epoch 8 Batch 1500 Loss 0.0078\n",
            "Epoch 8 Batch 1600 Loss 0.0055\n",
            "Epoch 8 Batch 1700 Loss 0.0055\n",
            "Epoch 8 Batch 1800 Loss 0.0055\n",
            "Epoch 8 Batch 1900 Loss 0.0081\n",
            "Epoch 8 Batch 2000 Loss 0.0078\n",
            "Epoch 8 Batch 2100 Loss 0.0095\n",
            "Epoch 8 Batch 2200 Loss 0.0050\n",
            "Epoch 8 Batch 2300 Loss 0.0086\n",
            "Epoch 8 Batch 2400 Loss 0.0054\n",
            "Epoch 8 Batch 2500 Loss 0.0100\n",
            "Epoch 8 Batch 2600 Loss 0.0062\n",
            "Epoch 8 Batch 2700 Loss 0.0090\n",
            "Epoch 8 Batch 2800 Loss 0.0085\n",
            "Epoch 8 Batch 2900 Loss 0.0075\n",
            "Epoch 8 Batch 3000 Loss 0.0071\n",
            "Epoch 8 Batch 3100 Loss 0.0066\n",
            "Epoch 8 Batch 3200 Loss 0.0098\n",
            "Epoch 8 Batch 3300 Loss 0.0076\n",
            "Epoch 8 Batch 3400 Loss 0.0084\n",
            "Epoch 8 Batch 3500 Loss 0.0081\n",
            "Epoch 8 Batch 3600 Loss 0.0077\n",
            "Epoch 8 Batch 3700 Loss 0.0092\n",
            "Epoch 8 Batch 3800 Loss 0.0061\n",
            "Epoch 8 Batch 3900 Loss 0.0081\n",
            "Epoch 8 Batch 4000 Loss 0.0087\n",
            "Epoch 8 Batch 4100 Loss 0.0102\n",
            "Epoch 8 Batch 4200 Loss 0.0071\n",
            "Epoch 8 Batch 4300 Loss 0.0078\n",
            "Epoch 8 Batch 4400 Loss 0.0081\n",
            "Epoch 8 Batch 4500 Loss 0.0066\n",
            "Epoch 8 Batch 4600 Loss 0.0068\n",
            "Epoch 8 Loss 0.0059 Val Loss 0.0137\n",
            "Time taken for 1 epoch 730.8820261955261 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0069\n",
            "Epoch 9 Batch 100 Loss 0.0094\n",
            "Epoch 9 Batch 200 Loss 0.0072\n",
            "Epoch 9 Batch 300 Loss 0.0038\n",
            "Epoch 9 Batch 400 Loss 0.0074\n",
            "Epoch 9 Batch 500 Loss 0.0064\n",
            "Epoch 9 Batch 600 Loss 0.0090\n",
            "Epoch 9 Batch 700 Loss 0.0078\n",
            "Epoch 9 Batch 800 Loss 0.0056\n",
            "Epoch 9 Batch 900 Loss 0.0052\n",
            "Epoch 9 Batch 1000 Loss 0.0065\n",
            "Epoch 9 Batch 1100 Loss 0.0057\n",
            "Epoch 9 Batch 1200 Loss 0.0061\n",
            "Epoch 9 Batch 1300 Loss 0.0057\n",
            "Epoch 9 Batch 1400 Loss 0.0071\n",
            "Epoch 9 Batch 1500 Loss 0.0067\n",
            "Epoch 9 Batch 1600 Loss 0.0075\n",
            "Epoch 9 Batch 1700 Loss 0.0073\n",
            "Epoch 9 Batch 1800 Loss 0.0053\n",
            "Epoch 9 Batch 1900 Loss 0.0064\n",
            "Epoch 9 Batch 2000 Loss 0.0056\n",
            "Epoch 9 Batch 2100 Loss 0.0076\n",
            "Epoch 9 Batch 2200 Loss 0.0061\n",
            "Epoch 9 Batch 2300 Loss 0.0086\n",
            "Epoch 9 Batch 2400 Loss 0.0069\n",
            "Epoch 9 Batch 2500 Loss 0.0083\n",
            "Epoch 9 Batch 2600 Loss 0.0075\n",
            "Epoch 9 Batch 2700 Loss 0.0086\n",
            "Epoch 9 Batch 2800 Loss 0.0088\n",
            "Epoch 9 Batch 2900 Loss 0.0065\n",
            "Epoch 9 Batch 3000 Loss 0.0069\n",
            "Epoch 9 Batch 3100 Loss 0.0061\n",
            "Epoch 9 Batch 3200 Loss 0.0076\n",
            "Epoch 9 Batch 3300 Loss 0.0049\n",
            "Epoch 9 Batch 3400 Loss 0.0094\n",
            "Epoch 9 Batch 3500 Loss 0.0070\n",
            "Epoch 9 Batch 3600 Loss 0.0081\n",
            "Epoch 9 Batch 3700 Loss 0.0072\n",
            "Epoch 9 Batch 3800 Loss 0.0085\n",
            "Epoch 9 Batch 3900 Loss 0.0059\n",
            "Epoch 9 Batch 4000 Loss 0.0055\n",
            "Epoch 9 Batch 4100 Loss 0.0069\n",
            "Epoch 9 Batch 4200 Loss 0.0062\n",
            "Epoch 9 Batch 4300 Loss 0.0055\n",
            "Epoch 9 Batch 4400 Loss 0.0095\n",
            "Epoch 9 Batch 4500 Loss 0.0087\n",
            "Epoch 9 Batch 4600 Loss 0.0066\n",
            "Epoch 9 Loss 0.0055 Val Loss 0.0139\n",
            "Time taken for 1 epoch 734.5921323299408 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0077\n",
            "Epoch 10 Batch 100 Loss 0.0051\n",
            "Epoch 10 Batch 200 Loss 0.0051\n",
            "Epoch 10 Batch 300 Loss 0.0075\n",
            "Epoch 10 Batch 400 Loss 0.0063\n",
            "Epoch 10 Batch 500 Loss 0.0073\n",
            "Epoch 10 Batch 600 Loss 0.0057\n",
            "Epoch 10 Batch 700 Loss 0.0053\n",
            "Epoch 10 Batch 800 Loss 0.0063\n",
            "Epoch 10 Batch 900 Loss 0.0054\n",
            "Epoch 10 Batch 1000 Loss 0.0050\n",
            "Epoch 10 Batch 1100 Loss 0.0064\n",
            "Epoch 10 Batch 1200 Loss 0.0062\n",
            "Epoch 10 Batch 1300 Loss 0.0069\n",
            "Epoch 10 Batch 1400 Loss 0.0083\n",
            "Epoch 10 Batch 1500 Loss 0.0082\n",
            "Epoch 10 Batch 1600 Loss 0.0068\n",
            "Epoch 10 Batch 1700 Loss 0.0065\n",
            "Epoch 10 Batch 1800 Loss 0.0068\n",
            "Epoch 10 Batch 1900 Loss 0.0063\n",
            "Epoch 10 Batch 2000 Loss 0.0054\n",
            "Epoch 10 Batch 2100 Loss 0.0063\n",
            "Epoch 10 Batch 2200 Loss 0.0070\n",
            "Epoch 10 Batch 2300 Loss 0.0056\n",
            "Epoch 10 Batch 2400 Loss 0.0091\n",
            "Epoch 10 Batch 2500 Loss 0.0062\n",
            "Epoch 10 Batch 2600 Loss 0.0073\n",
            "Epoch 10 Batch 2700 Loss 0.0065\n",
            "Epoch 10 Batch 2800 Loss 0.0075\n",
            "Epoch 10 Batch 2900 Loss 0.0075\n",
            "Epoch 10 Batch 3000 Loss 0.0083\n",
            "Epoch 10 Batch 3100 Loss 0.0083\n",
            "Epoch 10 Batch 3200 Loss 0.0085\n",
            "Epoch 10 Batch 3300 Loss 0.0076\n",
            "Epoch 10 Batch 3400 Loss 0.0071\n",
            "Epoch 10 Batch 3500 Loss 0.0062\n",
            "Epoch 10 Batch 3600 Loss 0.0082\n",
            "Epoch 10 Batch 3700 Loss 0.0070\n",
            "Epoch 10 Batch 3800 Loss 0.0093\n",
            "Epoch 10 Batch 3900 Loss 0.0078\n",
            "Epoch 10 Batch 4000 Loss 0.0080\n",
            "Epoch 10 Batch 4100 Loss 0.0058\n",
            "Epoch 10 Batch 4200 Loss 0.0079\n",
            "Epoch 10 Batch 4300 Loss 0.0059\n",
            "Epoch 10 Batch 4400 Loss 0.0065\n",
            "Epoch 10 Batch 4500 Loss 0.0064\n",
            "Epoch 10 Batch 4600 Loss 0.0058\n",
            "Epoch 10 Loss 0.0052 Val Loss 0.0142\n",
            "Early stopping triggered after 5 consecutive epochs w/o improvement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.embedding.variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ron5dNu1nG7a",
        "outputId": "9c2f9ad8-216a-4f86-d4c5-0ed25adcfad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Variable path=encoder_2/embedding_8/embeddings, shape=(166, 256), dtype=float32, value=[[-0.0377021  -0.13381055 -0.11647186 ...  0.02190447 -0.18612249\n",
              "    0.3392803 ]\n",
              "  [-0.03409722  0.14195894 -0.0854991  ... -0.07803918 -0.00742843\n",
              "    0.12786649]\n",
              "  [ 0.02340595 -0.08440623  0.10921772 ...  0.11545631 -0.07786053\n",
              "    0.02020929]\n",
              "  ...\n",
              "  [ 0.01179477  0.05889428  0.01100738 ...  0.03654375 -0.00295061\n",
              "   -0.02032111]\n",
              "  [ 0.09261505  0.00799699 -0.10922566 ...  0.16155736 -0.04701557\n",
              "   -0.1135793 ]\n",
              "  [ 0.02345644 -0.14304264  0.14749458 ... -0.05203421  0.19353712\n",
              "    0.01092249]]>]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.embedding.variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv3y11v-nLK0",
        "outputId": "6e83ec22-9367-41e5-d997-00c207111ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Variable path=decoder_6/embedding_9/embeddings, shape=(36, 256), dtype=float32, value=[[ 0.02532734  0.04688115  0.02036213 ...  0.02987245 -0.01590071\n",
              "    0.021944  ]\n",
              "  [-0.24319564  0.12953956 -0.02794108 ... -0.14007922  0.00697453\n",
              "   -0.16937147]\n",
              "  [ 0.02966861  0.01044544 -0.01499049 ... -0.01741641 -0.00173\n",
              "    0.04175289]\n",
              "  ...\n",
              "  [ 0.13381338 -0.06536956  0.08742485 ... -0.03918008 -0.1341787\n",
              "    0.12816505]\n",
              "  [ 0.10032396 -0.01028896  0.18520994 ...  0.02209266 -0.1323131\n",
              "    0.08644268]\n",
              "  [ 0.00735058  0.06271109  0.0967354  ... -0.05375795 -0.10891018\n",
              "   -0.0059194 ]]>]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bL2ukuAgrCrN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}